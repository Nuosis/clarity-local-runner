#!/usr/bin/env python3
"""
Performance Validation Test for Task 6.2.2: Send execution-log frames with ‚â§500ms latency

This test validates that the ExecutionLogService can send execution-log frames
to WebSocket connections within the required ‚â§500ms latency target.

Test Coverage:
- ExecutionLogService instantiation and basic functionality
- WebSocket broadcasting performance validation
- Latency measurement and verification
- Integration with structured logging system
- Error handling and graceful degradation

Performance Requirements:
- ‚â§500ms WebSocket latency for execution-log frames
- Consistent performance across multiple log entries
- Proper integration with existing WebSocket infrastructure
"""

import asyncio
import time
import sys
import os
from unittest.mock import AsyncMock, MagicMock, patch
from typing import Dict, Any

# Add the app directory to the Python path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

from services.execution_log_service import (
    ExecutionLogService, 
    LogEntryType, 
    get_execution_log_service,
    send_execution_log
)
from core.structured_logging import LogLevel


class ExecutionLogLatencyValidator:
    """Validator for ExecutionLogService performance and functionality."""
    
    def __init__(self):
        self.test_results = []
        self.correlation_id = "test_6_2_2_correlation"
        self.project_id = "test-project-123"
        self.execution_id = "exec_test_123"
        
    async def run_all_tests(self) -> Dict[str, Any]:
        """Run all validation tests and return results."""
        print("üöÄ Starting Task 6.2.2 ExecutionLogService Performance Validation")
        print("=" * 70)
        
        tests = [
            ("Service Instantiation", self.test_service_instantiation),
            ("Basic Log Broadcasting", self.test_basic_log_broadcasting),
            ("Latency Performance", self.test_latency_performance),
            ("Multiple Log Types", self.test_multiple_log_types),
            ("Error Handling", self.test_error_handling),
            ("Integration Points", self.test_integration_points),
        ]
        
        for test_name, test_func in tests:
            try:
                print(f"\nüìã Running: {test_name}")
                result = await test_func()
                self.test_results.append({
                    "test": test_name,
                    "status": "PASSED" if result["success"] else "FAILED",
                    "details": result
                })
                status_emoji = "‚úÖ" if result["success"] else "‚ùå"
                print(f"{status_emoji} {test_name}: {'PASSED' if result['success'] else 'FAILED'}")
                if not result["success"]:
                    print(f"   Error: {result.get('error', 'Unknown error')}")
                elif "performance" in result:
                    print(f"   Performance: {result['performance']:.2f}ms")
                    
            except Exception as e:
                self.test_results.append({
                    "test": test_name,
                    "status": "ERROR",
                    "error": str(e)
                })
                print(f"‚ùå {test_name}: ERROR - {str(e)}")
        
        return self.generate_summary()
    
    async def test_service_instantiation(self) -> Dict[str, Any]:
        """Test ExecutionLogService instantiation and basic setup."""
        try:
            # Test basic instantiation
            service = ExecutionLogService()
            assert service is not None, "Service should be instantiated"
            assert hasattr(service, 'logger'), "Service should have logger"
            
            # Test instantiation with correlation_id
            service_with_id = ExecutionLogService(correlation_id=self.correlation_id)
            assert service_with_id.correlation_id == self.correlation_id, "Correlation ID should be set"
            
            # Test factory function
            factory_service = get_execution_log_service(correlation_id=self.correlation_id)
            assert factory_service is not None, "Factory should create service"
            assert factory_service.correlation_id == self.correlation_id, "Factory should set correlation ID"
            
            return {
                "success": True,
                "message": "Service instantiation successful",
                "service_created": True,
                "correlation_id_set": True,
                "factory_function_works": True
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "Service instantiation failed"
            }
    
    async def test_basic_log_broadcasting(self) -> Dict[str, Any]:
        """Test basic execution log broadcasting functionality."""
        try:
            service = ExecutionLogService(correlation_id=self.correlation_id)
            
            # Mock the WebSocket broadcast function
            with patch('services.execution_log_service.broadcast_to_project') as mock_broadcast:
                mock_broadcast.return_value = AsyncMock()
                
                start_time = time.time()
                
                # Test basic log sending
                await service.send_execution_log(
                    project_id=self.project_id,
                    execution_id=self.execution_id,
                    log_entry_type=LogEntryType.INFO_LOG,
                    message="Test log message",
                    level=LogLevel.INFO
                )
                
                duration_ms = (time.time() - start_time) * 1000
                
                # Verify broadcast was called
                assert mock_broadcast.called, "broadcast_to_project should be called"
                call_args = mock_broadcast.call_args
                message = call_args[0][0]  # First argument is the message
                project_id = call_args[0][1]  # Second argument is project_id
                
                # Verify message structure
                assert message["type"] == "execution-log", "Message type should be execution-log"
                assert message["project_id"] == self.project_id, "Project ID should match"
                assert "payload" in message, "Message should have payload"
                assert message["payload"]["execution_id"] == self.execution_id, "Execution ID should match"
                assert message["payload"]["log_entry_type"] == LogEntryType.INFO_LOG.value, "Log entry type should match"
                assert message["payload"]["message"] == "Test log message", "Message should match"
                
                return {
                    "success": True,
                    "message": "Basic log broadcasting successful",
                    "performance": duration_ms,
                    "broadcast_called": True,
                    "message_structure_valid": True
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "Basic log broadcasting failed"
            }
    
    async def test_latency_performance(self) -> Dict[str, Any]:
        """Test latency performance against ‚â§500ms requirement."""
        try:
            service = ExecutionLogService(correlation_id=self.correlation_id)
            latencies = []
            target_latency_ms = 500
            
            # Mock the WebSocket broadcast function
            with patch('services.execution_log_service.broadcast_to_project') as mock_broadcast:
                mock_broadcast.return_value = AsyncMock()
                
                # Test multiple log sends to get average performance
                for i in range(10):
                    start_time = time.time()
                    
                    await service.send_execution_log(
                        project_id=self.project_id,
                        execution_id=f"{self.execution_id}_{i}",
                        log_entry_type=LogEntryType.OPERATION_LOG,
                        message=f"Performance test log {i}",
                        level=LogLevel.INFO,
                        additional_data={"test_iteration": i}
                    )
                    
                    duration_ms = (time.time() - start_time) * 1000
                    latencies.append(duration_ms)
                
                # Calculate performance metrics
                avg_latency = sum(latencies) / len(latencies)
                max_latency = max(latencies)
                min_latency = min(latencies)
                
                # Check if performance meets requirements
                performance_target_met = max_latency <= target_latency_ms
                
                return {
                    "success": performance_target_met,
                    "message": f"Latency performance {'meets' if performance_target_met else 'exceeds'} ‚â§500ms requirement",
                    "performance": avg_latency,
                    "avg_latency_ms": round(avg_latency, 2),
                    "max_latency_ms": round(max_latency, 2),
                    "min_latency_ms": round(min_latency, 2),
                    "target_latency_ms": target_latency_ms,
                    "performance_target_met": performance_target_met,
                    "test_iterations": len(latencies)
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "Latency performance test failed"
            }
    
    async def test_multiple_log_types(self) -> Dict[str, Any]:
        """Test different log entry types and specialized methods."""
        try:
            service = ExecutionLogService(correlation_id=self.correlation_id)
            
            with patch('services.execution_log_service.broadcast_to_project') as mock_broadcast:
                mock_broadcast.return_value = AsyncMock()
                
                # Test different log entry types
                test_cases = [
                    ("task_receipt", service.send_task_receipt_log, {
                        "project_id": self.project_id,
                        "execution_id": self.execution_id,
                        "task_id": "test_task_123",
                        "event_id": "test_event_123",
                        "event_type": "DEVTEAM_AUTOMATION"
                    }),
                    ("workflow_start", service.send_workflow_start_log, {
                        "project_id": self.project_id,
                        "execution_id": self.execution_id,
                        "workflow_type": "DEVTEAM_AUTOMATION",
                        "task_id": "test_task_123"
                    }),
                    ("workflow_complete", service.send_workflow_complete_log, {
                        "project_id": self.project_id,
                        "execution_id": self.execution_id,
                        "workflow_type": "DEVTEAM_AUTOMATION",
                        "task_id": "test_task_123",
                        "duration_ms": 1500.5
                    }),
                    ("workflow_error", service.send_workflow_error_log, {
                        "project_id": self.project_id,
                        "execution_id": self.execution_id,
                        "workflow_type": "DEVTEAM_AUTOMATION",
                        "error_message": "Test error message",
                        "task_id": "test_task_123"
                    })
                ]
                
                successful_tests = 0
                total_tests = len(test_cases)
                
                for test_name, method, kwargs in test_cases:
                    try:
                        start_time = time.time()
                        await method(**kwargs)
                        duration_ms = (time.time() - start_time) * 1000
                        
                        # Verify the method was called and broadcast occurred
                        assert mock_broadcast.called, f"{test_name} should trigger broadcast"
                        successful_tests += 1
                        
                    except Exception as e:
                        print(f"   Failed {test_name}: {str(e)}")
                
                success_rate = successful_tests / total_tests
                
                return {
                    "success": success_rate >= 0.8,  # 80% success rate required
                    "message": f"Multiple log types test: {successful_tests}/{total_tests} successful",
                    "successful_tests": successful_tests,
                    "total_tests": total_tests,
                    "success_rate": success_rate,
                    "broadcast_call_count": mock_broadcast.call_count
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "Multiple log types test failed"
            }
    
    async def test_error_handling(self) -> Dict[str, Any]:
        """Test error handling and graceful degradation."""
        try:
            service = ExecutionLogService(correlation_id=self.correlation_id)
            
            # Test with invalid project_id
            with patch('services.execution_log_service.broadcast_to_project') as mock_broadcast:
                mock_broadcast.return_value = AsyncMock()
                
                # Test with empty project_id (should handle gracefully)
                await service.send_execution_log(
                    project_id="",  # Invalid project_id
                    execution_id=self.execution_id,
                    log_entry_type=LogEntryType.ERROR_LOG,
                    message="Test error handling",
                    level=LogLevel.ERROR
                )
                
                # Should not call broadcast with invalid project_id
                assert not mock_broadcast.called, "Should not broadcast with invalid project_id"
                
                # Test with None project_id (using type ignore for testing)
                await service.send_execution_log(
                    project_id=None,  # type: ignore # Invalid project_id for testing
                    execution_id=self.execution_id,
                    log_entry_type=LogEntryType.ERROR_LOG,
                    message="Test error handling",
                    level=LogLevel.ERROR
                )
                
                # Should still not call broadcast
                assert not mock_broadcast.called, "Should not broadcast with None project_id"
                
                # Test with valid project_id but broadcast failure
                mock_broadcast.side_effect = Exception("Broadcast failed")
                
                # This should not raise an exception (graceful degradation)
                await service.send_execution_log(
                    project_id=self.project_id,
                    execution_id=self.execution_id,
                    log_entry_type=LogEntryType.ERROR_LOG,
                    message="Test error handling",
                    level=LogLevel.ERROR
                )
                
                return {
                    "success": True,
                    "message": "Error handling successful",
                    "invalid_project_id_handled": True,
                    "broadcast_failure_handled": True,
                    "graceful_degradation": True
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "Error handling test failed"
            }
    
    async def test_integration_points(self) -> Dict[str, Any]:
        """Test integration with existing systems."""
        try:
            # Test utility function
            with patch('services.execution_log_service.broadcast_to_project') as mock_broadcast:
                mock_broadcast.return_value = AsyncMock()
                
                start_time = time.time()
                
                # Test utility function
                await send_execution_log(
                    project_id=self.project_id,
                    execution_id=self.execution_id,
                    log_entry_type=LogEntryType.INFO_LOG,
                    message="Integration test",
                    level=LogLevel.INFO,
                    correlation_id=self.correlation_id
                )
                
                duration_ms = (time.time() - start_time) * 1000
                
                # Verify broadcast was called
                assert mock_broadcast.called, "Utility function should trigger broadcast"
                
                # Test message envelope format compliance
                call_args = mock_broadcast.call_args
                message = call_args[0][0]
                
                # Verify envelope format: { type, ts, project_id, payload }
                required_fields = ["type", "ts", "project_id", "payload"]
                for field in required_fields:
                    assert field in message, f"Message should have {field} field"
                
                # Verify payload structure
                payload = message["payload"]
                payload_fields = ["execution_id", "log_entry_type", "level", "message", "timestamp"]
                for field in payload_fields:
                    assert field in payload, f"Payload should have {field} field"
                
                return {
                    "success": True,
                    "message": "Integration points successful",
                    "performance": duration_ms,
                    "utility_function_works": True,
                    "envelope_format_compliant": True,
                    "payload_structure_valid": True
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "Integration points test failed"
            }
    
    def generate_summary(self) -> Dict[str, Any]:
        """Generate test summary and results."""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result["status"] == "PASSED")
        failed_tests = sum(1 for result in self.test_results if result["status"] == "FAILED")
        error_tests = sum(1 for result in self.test_results if result["status"] == "ERROR")
        
        success_rate = passed_tests / total_tests if total_tests > 0 else 0
        overall_success = success_rate >= 0.8  # 80% success rate required
        
        # Extract performance data
        performance_data = []
        for result in self.test_results:
            if "details" in result and "performance" in result["details"]:
                performance_data.append(result["details"]["performance"])
        
        avg_performance = sum(performance_data) / len(performance_data) if performance_data else 0
        
        print(f"\n{'='*70}")
        print("üìä TASK 6.2.2 VALIDATION SUMMARY")
        print(f"{'='*70}")
        print(f"Total Tests: {total_tests}")
        print(f"‚úÖ Passed: {passed_tests}")
        print(f"‚ùå Failed: {failed_tests}")
        print(f"‚ö†Ô∏è  Errors: {error_tests}")
        print(f"Success Rate: {success_rate:.1%}")
        
        if performance_data:
            print(f"Average Performance: {avg_performance:.2f}ms")
            print(f"Performance Target: ‚â§500ms")
            print(f"Performance Status: {'‚úÖ MEETS TARGET' if avg_performance <= 500 else '‚ùå EXCEEDS TARGET'}")
        
        print(f"\nOverall Status: {'‚úÖ PASSED' if overall_success else '‚ùå FAILED'}")
        
        if overall_success:
            print("\nüéâ Task 6.2.2 ExecutionLogService implementation is VALIDATED!")
            print("   - Execution-log frames are sent with ‚â§500ms latency")
            print("   - WebSocket broadcasting integration is functional")
            print("   - Structured logging integration is working")
            print("   - Error handling and graceful degradation implemented")
        else:
            print("\n‚ö†Ô∏è  Task 6.2.2 validation has issues that need attention.")
        
        return {
            "overall_success": overall_success,
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "error_tests": error_tests,
            "success_rate": success_rate,
            "average_performance_ms": avg_performance,
            "performance_target_met": avg_performance <= 500 if performance_data else None,
            "test_results": self.test_results
        }


async def main():
    """Main test execution function."""
    validator = ExecutionLogLatencyValidator()
    results = await validator.run_all_tests()
    
    # Exit with appropriate code
    exit_code = 0 if results["overall_success"] else 1
    sys.exit(exit_code)


if __name__ == "__main__":
    asyncio.run(main())